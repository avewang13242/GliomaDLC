{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "base_path = \"./Data_122824/\"\n",
    "tr_folder_path = os.path.join(base_path, \"Glioma_MDC_2025_training_normalized\")\n",
    "te_folder_path = os.path.join(base_path, \"Glioma_MDC_2025_test_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({(512, 512): 1439}, {(512, 512): 200})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to check image sizes\n",
    "def check_image_sizes(folder_path):\n",
    "    sizes = {}\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".jpg\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with Image.open(file_path) as img:\n",
    "                size = img.size  # (width, height)\n",
    "                sizes[file_name] = size\n",
    "    return sizes\n",
    "\n",
    "# Check image sizes in each folder\n",
    "training_image_sizes = check_image_sizes(tr_folder_path)\n",
    "testing_image_sizes = check_image_sizes(te_folder_path)\n",
    "\n",
    "training_image_sizes_summary = {size: list(training_image_sizes.values()).count(size) for size in set(training_image_sizes.values())}\n",
    "testing_image_sizes_summary = {size: list(testing_image_sizes.values()).count(size) for size in set(testing_image_sizes.values())}\n",
    "\n",
    "training_image_sizes_summary, testing_image_sizes_summary\n",
    "\n",
    "# All images are of 512 by 512 and need to be resized to 1015x1015 to fit JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_points_and_bboxes_with_crops(image, rois):\n",
    "    \"\"\"\n",
    "    Overlay points and bounding boxes and return crops.\n",
    "    \"\"\"\n",
    "    overlay_image = image.copy()\n",
    "    crops = [] \n",
    "    for points in rois:\n",
    "        points = np.array(points, dtype=np.int32)\n",
    "        cv2.polylines(overlay_image, [points], isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "\n",
    "        x_min, y_min = np.min(points, axis=0)\n",
    "        x_max, y_max = np.max(points, axis=0)\n",
    "        cv2.rectangle(overlay_image, (x_min, y_min), (x_max, y_max), color=(0, 0, 255), thickness=2)\n",
    "\n",
    "        crop = image[y_min:y_max, x_min:x_max]\n",
    "        crops.append(crop)\n",
    "\n",
    "    return overlay_image, crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rescale coordinates if needed\n",
    "def rescale_coordinates(points, original_size, target_size):\n",
    "    \"\"\"\n",
    "    Rescale the points from the original size to the target size.\n",
    "    \"\"\"\n",
    "    if original_size != target_size:\n",
    "        scale_factor = target_size / original_size\n",
    "        return [(int(x * scale_factor), int(y * scale_factor)) for x, y in points]\n",
    "    else:\n",
    "        return [(int(x), int(y)) for x, y in points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crops_image(image_path, json_path):\n",
    "    \"\"\"\n",
    "    Plot the original image, overlayed image with bounding boxes, and cropped regions.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Determine if rescaling is needed\n",
    "    json_image_height = data['imageHeight']\n",
    "    json_image_width = data['imageWidth']\n",
    "    actual_image_height, actual_image_width, _ = image.shape\n",
    "\n",
    "    # Rescale coordinates if the JSON dimensions differ from the actual image dimensions\n",
    "    shapes = data['shapes']\n",
    "    rois = [\n",
    "        rescale_coordinates(shape['points'], json_image_height, actual_image_height) \n",
    "        for shape in shapes\n",
    "    ]\n",
    "\n",
    "    # Overlay the ROIs and bounding boxes on the image, and get cropped regions\n",
    "    overlay_image, crops = overlay_points_and_bboxes_with_crops(image, rois)\n",
    "    return overlay_image, crops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7cb4c59c8eb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAGdCAYAAADpHzMaAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAI/VJREFUeJzt3XuMlOX99/HPnGf2NLDAnipQtFpbFZpYocRq7M8NhyZELGnU9g80RlO7mODG2JBUqW2TjZq0pobiPy3UpGhrUjWahkaxLGkKNGKMMWn5CaEP+OCusro7u7M75/v5w7J9huNeX2aZ5fL9SiZhd+fLde099364MzN8NhQEQSAAgFfC9d4AAKD2CHcA8BDhDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADwUrfcGTlWpVHT8+HE1NzcrFArVezsAMKMEQaDR0VF1dXUpHD779fmMC/fjx49r/vz59d4GAMxox44d02WXXXbWr8+4cG9ubpYkPd79CyWjKafZppTb/SVpfHTCeUaSihM509x4vmCaC0UjzjPJphbTWpF4zDSXLZScZ0ayY6a1rOJJ9+8tGrE9exmN2uZmtdget3nzWp1nmlMNprUUFE1jY59+4jxz/P8eM601kbP9rLW2uh9HSWpJzzbNuZoojOuHO9ZPZuXZTFu4b9myRU899ZQGBga0ZMkSPfPMM1q6dOl5504+FZOMppSKuYV1KuZ+ogbGIxCJ2p4yqpTdQ1oyhrvjP44nRWJx01y54h7uuWjZtJZVPGoId2NIx4xzlvNYkhoSjc4zjYYZSeZwr8TdL4qSjjlwUlCyHn/beg1x4z+URud72npaXlD9wx/+oN7eXm3evFlvv/22lixZopUrV+qjjz6ajuUAAKeYlnD/xS9+ofvuu0/33HOPvvrVr+rZZ59VQ0ODfvvb307HcgCAU9Q83AuFgg4cOKDu7u7/LhIOq7u7W3v37q31cgCAM6j5c+4nTpxQuVxWe3t71efb29v1r3/967T75/N55fP5yY8zmUyttwQAnzt1/09MfX19SqfTkzfeBgkAF67m4T537lxFIhENDg5WfX5wcFAdHR2n3X/Tpk0aGRmZvB07ZnvbEwDgv2oe7vF4XNdff7127do1+blKpaJdu3Zp+fLlp90/kUiopaWl6gYAuDDT8j733t5erV+/Xl//+te1dOlSPf3008pms7rnnnumYzkAwCmmJdzvuOMOffzxx3rsscc0MDCgr33ta9q5c+dpL7ICAKbHtP0P1Q0bNmjDhg3T9dcDAM5hxnXLnDSrudH5v2EnYknndSIlW41ANjCNqRKyvcxRlvuCYxPjprXGPjlhmhsec+/pGS/kz3+nM7B0xEg6Z4ve2SSMa7W0NJnmEkn381iSMuNZ5xnL8ZCkVMw213SePpQzaWy0VSRMTNjOrbGs+3GUpFjc/XGzvMYYrUytiqTub4UEANQe4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4CHCHQA8RLgDgIcIdwDwEOEOAB4i3AHAQ4Q7AHhoxhaHNaSb1RB3KwzKTRSc1ymGbQ1gRVVMc7lKyTSXL7p/b59mx0xrnRgeMc3lSmXnmXDcVso1NmorRSsWi84zsdjUippONV6YbZorBbZzqxC4n1ulivtjJkmtTbZStIVdbc4zubztsc7l3R9rScobzmNJCgL3LEk2uJeiTbE3jCt3APAR4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4CHCHQA8RLgDgIcIdwDwEOEOAB4i3AHAQ4Q7AHiIcAcAD83YVsiRQkEFx+2NTbi3x02M25oTS2Vb49xI3rbex58OO8+M5SdMaxVla8osGwoeS+Wcaa1KxdacGE8mnGdKxrWGMsOmuUrINKaCpeExbLu+i8fjprlQ2D1y5sybZ1qrULI9btkJ2zmZjCWdZz4dyTjPjBemlnNcuQOAhwh3APAQ4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4CHCHQA8RLgDgIcIdwDwEOEOAB4i3AHAQ4Q7AHhoxrZCfvzJkJIxt5bHwPBvVbFScp6RpPGCrXFxJOfeXClJEyX3prpiYGvFKxv/ya8YyiSLge34B7JVJ6bi7tWV0ZDtgBSLtubQ0aytObRiaPNMpRpNa7XNNY1JEfdj2ZxuMS01nBk1zX089IlpLltx/9mORt3bNSeKtEICwOcW4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4CHCHQA8RLgDgIcIdwDwEOEOAB4i3AHAQ4Q7AHhoxhaHjY+Mqhx1K15KNTY4r5PP551nJCmbzZrmxsZspVDFsnvBViTmXpIlSZFoxDRnqSkLlW2nYDyetM0lE84z5aKx3MzwmEky1H99plhyXy9XsJ3/4zlbcV5mzL3Ma95sW3FYIuH+WEv2TMhNuM8lEin3dYpTO/ZcuQOAhwh3APAQ4Q4AHqp5uP/kJz9RKBSqul199dW1XgYAcA7T8oLqNddcozfeeOO/i0Rn7Ou2AOClaUndaDSqjo6O6firAQBTMC3Pub///vvq6urS5Zdfru9///s6evToWe+bz+eVyWSqbgCAC1PzcF+2bJm2b9+unTt3auvWrTpy5IhuuukmjY6e+f2tfX19SqfTk7f58+fXeksA8LlT83BfvXq1vvvd72rx4sVauXKl/vznP2t4eFh//OMfz3j/TZs2aWRkZPJ27NixWm8JAD53pv2VzlmzZumqq67SoUOHzvj1RCJh/p9kAIAzm/b3uY+Njenw4cPq7Oyc7qUAAP9R83B/+OGH1d/fr3//+9/6+9//rttvv12RSER33XVXrZcCAJxFzZ+W+eCDD3TXXXdpaGhI8+bN0ze/+U3t27dP8+bNq/VSAICzqHm4v/DCCzX5e6JBoFjg1o8XMdTpWRvgxsbHTXPWFr5IzP2hiqfcG+ckSRexFTJm7EBMJt0bQCXb421tAK1ULEdESjc1m+bi8bjzjPX8HxoaMs0NNrm/vhZV2bRWLpczzQWBbT2LkiEPSqWpzdAtAwAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAhwh3APAQ4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4KFp/01MVkkFSjo2BpYLBed1JiZsjX8TeVvjXDmwNQVGI4amxojt3+5iuWSaK1XcGx7DxgZKa5thdmLCecba5BkOG49/xdZKGBTdz/9I3nb8C4aftc/m3H9uPvnkE9Nanw4Nm+aKxaJpLhp2j9Ny2f2xnmrbKFfuAOAhwh0APES4A4CHCHcA8BDhDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAhwh3APAQ4Q4AHpqxrZBBvqig7NZYl8u7N9VlR8ecZyR7U6CtE1KKxNwfqlLJ1u6YGRs1zRVK7g138caUaa1wyHbqRgztmk1NTaa1rCxNgZLt8W5uaDSt1draapqbM2eO80xh3HY+ZjLDprlcztb42pB0P5Yxw/lYrkxthit3APAQ4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4CHCHQA8RLgDgIcIdwDwEOEOAB4i3AHAQ4Q7AHhoxhaHhStlhStuBUqF3ITzOmPZjPOMJBUrtgqwZIOthCqWTDjP5PJF01pZw3GUpFLZ/ZhEknHTWqlGW+FVOOpe1BQYvi9JCgWBaU4V21xgWK/ReBxnz55tmmtra3OeGT5hPI5G1sK9aNQ9TlNx95/rcDE0tfs5/80AgBmPcAcADxHuAOAhwh0APES4A4CHCHcA8BDhDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAh2ZsK2QiElUi4ra9StmtRVKScrmc84wkFQ3tgpLUFI+Z5pLJpPOMscxQodDUWudOFQ67XyskEu6teJLU1GRr15yYcG+8zI6Pm9aKRWznSEMyZZpLRN3PrUTc1sppba5Mpdy/t0Kj7XjEE7Z4i9hOfyVj7us1N7qfx5HC1H7OuHIHAA8R7gDgIedw37Nnj9asWaOuri6FQiG9/PLLVV8PgkCPPfaYOjs7lUql1N3drffff79W+wUATIFzuGezWS1ZskRbtmw549effPJJ/epXv9Kzzz6r/fv3q7GxUStXrjQ/tw0AcOf8CsDq1au1evXqM34tCAI9/fTT+vGPf6zbbrtNkvTcc8+pvb1dL7/8su68884L2y0AYEpq+pz7kSNHNDAwoO7u7snPpdNpLVu2THv37j3jTD6fVyaTqboBAC5MTcN9YGBAktTe3l71+fb29smvnaqvr0/pdHryNn/+/FpuCQA+l+r+bplNmzZpZGRk8nbs2LF6bwkALnk1DfeOjg5J0uDgYNXnBwcHJ792qkQioZaWlqobAODC1DTcFy1apI6ODu3atWvyc5lMRvv379fy5ctruRQA4Byc3y0zNjamQ4cOTX585MgRvfPOO2ptbdWCBQu0ceNG/fznP9eVV16pRYsW6dFHH1VXV5fWrl1by30DAM7BOdzfeustfetb35r8uLe3V5K0fv16bd++XY888oiy2azuv/9+DQ8P65vf/KZ27txp6kYBANg4h/stt9yiIDh7aVAoFNJPf/pT/fSnP72gjQEA7GZsK+SsWbOUirm1wf176GPndawv4I5V3BsoJVtzovTZ/wdwNTo6alrL2gqZTrsfy+aGRtNa8ajt1M0YjmPRMCNJIUNLoyTlK7ZzJJpyb6GMR22tnHHHxtaTwnI/tywzkq1xUZJyadvjbfnZLleKzjOVKc7U/a2QAIDaI9wBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAhwh3APAQ4Q4AHiLcAcBDhDsAeIhwBwAPzdjisEK5pEio5DRzrrbKs8kZS6GiKVuFcSrlVoZ2UijiXgoVi9mKq2Y128rUEqkG5xnLYyZJYxlbKVoxX3CeiYRs10DJWNw0F4/a5sKGwrdCdsK01qeffGKaG/rYvdwvPz5mWqtctpX7RUK2czJsmItMY5EaV+4A4CHCHQA8RLgDgIcIdwDwEOEOAB4i3AHAQ4Q7AHiIcAcADxHuAOAhwh0APES4A4CHCHcA8BDhDgAemrGtkKFITKGoW6th3NJKOPKp84wkxY2Ni9Go7ZCX5d44Fw7b/u1uSCZMcynD8S+U3Jo/TxrPjpvmQhX34xgPGR/rkO2xDhv2KElBwf1YFidyprUmMramxmLefb2o8RK0ucH9fJQkFW1tklFDm6elObQSntrjzJU7AHiIcAcADxHuAOAhwh0APES4A4CHCHcA8BDhDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAh2ZsK2S+UlG4UnGaqSjkvE48nnSekSSV3fZ2UjFfMM2FIu7/Drsfjf8wthJa5qLGXUaMjZeppOHxrtj2GJRs7YKFQt40Z2kqDRkf6pix3TQZd28cjYWNTarNtp9RBbaDEg1F3GfC7o9ZEJ7aecWVOwB4iHAHAA8R7gDgIcIdADxEuAOAhwh3APAQ4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4CHCHQA8RLgDgIdmbCtkKRRRybFlrRJ2b2WLGZr0JGk0N2Gak7Hhrjnd4jyTMjTwSdJ4PmeaGyuOmuZMAltTYyoWd56xNPdJUjFna3fMByXTXMxwrWY5HpKUitrmSgX3cys0xRbEU1WKtgbWsLEp09JUaik3DU3x1OfKHQA8RLgDgIecw33Pnj1as2aNurq6FAqF9PLLL1d9/e6771YoFKq6rVq1qlb7BQBMgXO4Z7NZLVmyRFu2bDnrfVatWqUPP/xw8vb8889f0CYBAG6cX91bvXq1Vq9efc77JBIJdXR0mDcFALgw0/Kc++7du9XW1qYvf/nLeuCBBzQ0NHTW++bzeWUymaobAODC1DzcV61apeeee067du3SE088of7+fq1evVrl8pnfztTX16d0Oj15mz9/fq23BACfOzV/n/udd945+efrrrtOixcv1hVXXKHdu3fr1ltvPe3+mzZtUm9v7+THmUyGgAeACzTtb4W8/PLLNXfuXB06dOiMX08kEmppaam6AQAuzLSH+wcffKChoSF1dnZO91IAgP9wflpmbGys6ir8yJEjeuedd9Ta2qrW1lY9/vjjWrdunTo6OnT48GE98sgj+tKXvqSVK1fWdOMAgLNzDve33npL3/rWtyY/Pvl8+fr167V161a9++67+t3vfqfh4WF1dXVpxYoV+tnPfqZEwtZzAgBw5xzut9xyi4Lg7M06f/nLXy5oQyel581TKt7gNPOFmPvrw0P/O+Y8I0nBeNY0VykZS6Ei7t9bLGJ71q1YLJrmJibcS6EKBVu5UzyWNM2FDIVXSWO5VlS2crO48dnSVNL9mFgLwIJyxTQ39PEJ55nGBtv7PooFW+FYENjmQiHbsZwudMsAgIcIdwDwEOEOAB4i3AHAQ4Q7AHiIcAcADxHuAOAhwh0APES4A4CHCHcA8BDhDgAeItwBwEOEOwB4qOa/Zq9Wygqr7Phvz6y585zXmXPCfUaSSmf5nbDnUyjZ5iYmJpxnYjFbzXJDg1sb50kRQ3Pl6Oioaa3Q2YtJz63i3mZYytuaK2VsTmyI2x63WU3uv8UsaVzL2m6aGR52nomGmkxrWc//IBIxzZ3t90SfSy7n3qQ6UZhaFnDlDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAhwh3APAQ4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4KEZ2wp56OhRJaIpp5nZc1qd15nbZmuFLJZtTYEnTnximsuOZJxnkqlG01pz5841zbWk3Nv7IrbiRJWLtsFQ2b1OMl/I2tYq2faYbGo2zTXEY84zqZgtAkIV2/k/nnVvQZzV4t52KUnRZNw0F9iKWzUx4f69jY6MuK9TpBUSAD63CHcA8BDhDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAhwh3APAQ4Q4AHiLcAcBDM7Y47OPMmOKRktNMPnAvhZrdknCekaTW2WnTXMg0JX00eMJ5pjA2blorI1u5WTgccZ4pTuRNa6VSbqVyJwVl9zKvSMS9kEuSUinbXEtTg2kuKvfvrVK0nSOhkPvPmiRFou6RMzZaNK2VsB1+hcq2a958Zsx5JlZxT4TiFGe4cgcADxHuAOAhwh0APES4A4CHCHcA8BDhDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAhwh3APDQjG2FPPHpJ4qF3RobM6PDzusUc83OM5I0q8nWStg2Z65prjXd6jyTnyiY1srnbHOjo1nnmXLRrfnzpGjS1q+ZbGh0nmlpdp+RpEZjc2Uy6t6uKUlhQyukyrbjXy6XTXPZ8ZzzTKVi+L4kxQ0tpZIUMVa3VkruxyQRcd9jKTy1a3Ku3AHAQ4Q7AHjIKdz7+vp0ww03qLm5WW1tbVq7dq0OHjxYdZ9cLqeenh7NmTNHTU1NWrdunQYHB2u6aQDAuTmFe39/v3p6erRv3z69/vrrKhaLWrFihbLZ/z7X+tBDD+nVV1/Viy++qP7+fh0/flzf+c53ar5xAMDZOb2gunPnzqqPt2/frra2Nh04cEA333yzRkZG9Jvf/EY7duzQ//zP/0iStm3bpq985Svat2+fvvGNb9Ru5wCAs7qg59xHRkYkSa2tn72T48CBAyoWi+ru7p68z9VXX60FCxZo7969Z/w78vm8MplM1Q0AcGHM4V6pVLRx40bdeOONuvbaayVJAwMDisfjmjVrVtV929vbNTAwcMa/p6+vT+l0evI2f/5865YAAP9hDveenh699957euGFFy5oA5s2bdLIyMjk7dixYxf09wEAjP+JacOGDXrttde0Z88eXXbZZZOf7+joUKFQ0PDwcNXV++DgoDo6Os74dyUSCSUSbv9ZCQBwbk5X7kEQaMOGDXrppZf05ptvatGiRVVfv/766xWLxbRr167Jzx08eFBHjx7V8uXLa7NjAMB5OV259/T0aMeOHXrllVfU3Nw8+Tx6Op1WKpVSOp3Wvffeq97eXrW2tqqlpUUPPvigli9fzjtlAOAicgr3rVu3SpJuueWWqs9v27ZNd999tyTpl7/8pcLhsNatW6d8Pq+VK1fq17/+dU02CwCYGqdwD4LgvPdJJpPasmWLtmzZYt4UAODCzNhWyGIxqyBcdJspuLfHDRRs76uPd3Wa5trnnfmF5fOZPXuO80yhYGv8GzrxiWkunhx2nkk22JoTYxHbqdtsaIWclbY1hybjcdNcyNjUWMiNO8+Uim4/YydZWyGLeffG0ULUvUlSkgq5CdNcPGp7E6GlTDJiaIWMVGiFBIDPLcIdADxEuAOAhwh3APAQ4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4CHCHQA8RLgDgIcIdwDw0IwtDotGK4qFHYvAKudvrTxVLjvmPCNJY2O2uXzeVtQUjbo/VMmEe0mWJIWM/+Y3NLkXbE2lafRMhoaGTHPRsHu9U9FY5FXM2gqvgqJtvXLJvZTLWlIWMj5uiYR7mVpjyvab2poakqY5yzkiSXnDsYxG3NeKVqY2w5U7AHiIcAcADxHuAOAhwh0APES4A4CHCHcA8BDhDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAh2ZsK2RQKSqQW2NaLBxxXidkaFuUpHLB1u44OjximhtpcG94TDa4tzRK9qbGWCzmPhS2XV9ELGtJKhbcmxrHx8dNawVl95ZGSYqGbMckHnGfixvPf2NxomKGuaaktRUyZZqLGL+5cmHCeSYk95+1UGhqM1y5A4CHCHcA8BDhDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAhwh3APAQ4Q4AHiLcAcBDhDsAeGjGtkImY1HFwm7bi0Xcv51oKO48Y11Lkop5W1Pgp59+6jwTG8+b1ioHFdPcRN69KTOfN+6xXDbNFUvu61WKJdNasSm2950qmkja1ou7t6JGDE2qkhSV7RwJJtybE4t59yZPSSoY5+JR2zVvUHY/T0Ix97VCUyyt5ModADxEuAOAhwh3APAQ4Q4AHiLcAcBDhDsAeIhwBwAPEe4A4CHCHQA8RLgDgIcIdwDwEOEOAB4i3AHAQzO2FTIRjysWSTjNhA0lfLGK7d+3ZNxtbyeFplrpdoqxsTHnmfLouGktayvkaNa98W9kNGNaK5m0NScqcG+TDFWM7Y62wkUVcramzGLOfcFEzNaKmjBeFsZK7s2h4xnb+fip8WctmYiZ5ioV91bIZDzlPBOZ4vfFlTsAeIhwBwAPOYV7X1+fbrjhBjU3N6utrU1r167VwYMHq+5zyy23KBQKVd1+8IMf1HTTAIBzcwr3/v5+9fT0aN++fXr99ddVLBa1YsUKZbPZqvvdd999+vDDDydvTz75ZE03DQA4N6cXVHfu3Fn18fbt29XW1qYDBw7o5ptvnvx8Q0ODOjo6arNDAICzC3rOfWRkRJLU2tpa9fnf//73mjt3rq699lpt2rRJ4+Nnf9dGPp9XJpOpugEALoz5rZCVSkUbN27UjTfeqGuvvXby89/73ve0cOFCdXV16d1339WPfvQjHTx4UH/605/O+Pf09fXp8ccft24DAHAG5nDv6enRe++9p7/97W9Vn7///vsn/3zdddeps7NTt956qw4fPqwrrrjitL9n06ZN6u3tnfw4k8lo/vz51m0BAGQM9w0bNui1117Tnj17dNlll53zvsuWLZMkHTp06IzhnkgklEjY/kMQAODMnMI9CAI9+OCDeumll7R7924tWrTovDPvvPOOJKmzs9O0QQCAO6dw7+np0Y4dO/TKK6+oublZAwMDkqR0Oq1UKqXDhw9rx44d+va3v605c+bo3Xff1UMPPaSbb75ZixcvnpZvAABwOqdw37p1q6TP/qPS/2/btm26++67FY/H9cYbb+jpp59WNpvV/PnztW7dOv34xz+u2YYBAOfn/LTMucyfP1/9/f0XtKGTyqVA4YpbYdCEoXBpVqLReUaS4lHba9GRsO3dpznD95YvupdkSVLZ1rdk2uPEhHvZmPTZW2gtwiFDCVjZVlwVVNxLsiQpXLYVlaXi7udWc7LBtFZjylY4Nq/Rfb2c8RyxlMRJUrHBVkqXMrx2GDK0y4UqU5uhWwYAPES4A4CHCHcA8BDhDgAeItwBwEOEOwB4iHAHAA8R7gDgIcIdADxEuAOAhwh3APAQ4Q4AHjL/JqbpcrKcrFgpOM9aZgoV2yHIl3OmuVDJVriULxuKw8q24qSKsTisYNhjsWIsADNel5iKwxwL7E4KKiXTXLhiKw6LVNyPSWGKJVSnihrLzXIl9z2GKrbzOCjZHreQ7WFTEHZfL1Zwz4Px4mdFaucrcpxx4T46OipJevX/bKvzTgBg5hodHVU6nT7r10PB+eL/IqtUKjp+/Liam5sVClVfQp78/arHjh1TS0tLnXY4s3BMqnE8TscxqXapH48gCDQ6Oqquri6Fz1EhPuOu3MPh8Hl/L2tLS8sl+aBMJ45JNY7H6Tgm1S7l43GuK/aTeEEVADxEuAOAhy6pcE8kEtq8ebMShl9n5SuOSTWOx+k4JtU+L8djxr2gCgC4cJfUlTsAYGoIdwDwEOEOAB4i3AHAQ5dUuG/ZskVf/OIXlUwmtWzZMv3jH/+o95bq4ic/+YlCoVDV7eqrr673ti6qPXv2aM2aNerq6lIoFNLLL79c9fUgCPTYY4+ps7NTqVRK3d3dev/99+uz2YvgfMfj7rvvPu2cWbVqVX02exH09fXphhtuUHNzs9ra2rR27VodPHiw6j65XE49PT2aM2eOmpqatG7dOg0ODtZpx7V3yYT7H/7wB/X29mrz5s16++23tWTJEq1cuVIfffRRvbdWF9dcc40+/PDDydvf/va3em/pospms1qyZIm2bNlyxq8/+eST+tWvfqVnn31W+/fvV2Njo1auXKlczlb4NtOd73hI0qpVq6rOmeeff/4i7vDi6u/vV09Pj/bt26fXX39dxWJRK1asUDabnbzPQw89pFdffVUvvvii+vv7dfz4cX3nO9+p465rLLhELF26NOjp6Zn8uFwuB11dXUFfX18dd1UfmzdvDpYsWVLvbcwYkoKXXnpp8uNKpRJ0dHQETz311OTnhoeHg0QiETz//PN12OHFderxCIIgWL9+fXDbbbfVZT8zwUcffRRICvr7+4Mg+Ox8iMViwYsvvjh5n3/+85+BpGDv3r312mZNXRJX7oVCQQcOHFB3d/fk58LhsLq7u7V379467qx+3n//fXV1denyyy/X97//fR09erTeW5oxjhw5ooGBgarzJZ1Oa9myZZ/b80WSdu/erba2Nn35y1/WAw88oKGhoXpv6aIZGRmRJLW2tkqSDhw4oGKxWHWOXH311VqwYIE358glEe4nTpxQuVxWe3t71efb29s1MDBQp13Vz7Jly7R9+3bt3LlTW7du1ZEjR3TTTTdN1iV/3p08Jzhf/mvVqlV67rnntGvXLj3xxBPq7+/X6tWrVTZ2/l9KKpWKNm7cqBtvvFHXXnutpM/OkXg8rlmzZlXd16dzZMa1QuL8Vq9ePfnnxYsXa9myZVq4cKH++Mc/6t57763jzjBT3XnnnZN/vu6667R48WJdccUV2r17t2699dY67mz69fT06L333vvcvS51SVy5z507V5FI5LRXsgcHB9XR0VGnXc0cs2bN0lVXXaVDhw7VeyszwslzgvPl7C6//HLNnTvX+3Nmw4YNeu211/TXv/61qkq8o6NDhUJBw8PDVff36Ry5JMI9Ho/r+uuv165duyY/V6lUtGvXLi1fvryOO5sZxsbGdPjwYXV2dtZ7KzPCokWL1NHRUXW+ZDIZ7d+/n/PlPz744AMNDQ15e84EQaANGzbopZde0ptvvqlFixZVff36669XLBarOkcOHjyoo0ePenOOXDJPy/T29mr9+vX6+te/rqVLl+rpp59WNpvVPffcU++tXXQPP/yw1qxZo4ULF+r48ePavHmzIpGI7rrrrnpv7aIZGxuruuo8cuSI3nnnHbW2tmrBggXauHGjfv7zn+vKK6/UokWL9Oijj6qrq0tr166t36an0bmOR2trqx5//HGtW7dOHR0dOnz4sB555BF96Utf0sqVK+u46+nT09OjHTt26JVXXlFzc/Pk8+jpdFqpVErpdFr33nuvent71draqpaWFj344INavny5vvGNb9R59zVS77fruHjmmWeCBQsWBPF4PFi6dGmwb9++em+pLu64446gs7MziMfjwRe+8IXgjjvuCA4dOlTvbV1Uf/3rXwNJp93Wr18fBMFnb4d89NFHg/b29iCRSAS33nprcPDgwfpuehqd63iMj48HK1asCObNmxfEYrFg4cKFwX333RcMDAzUe9vT5kzHQlKwbdu2yftMTEwEP/zhD4PZs2cHDQ0Nwe233x58+OGH9dt0jVH5CwAeuiSecwcAuCHcAcBDhDsAeIhwBwAPEe4A4CHCHQA8RLgDgIcIdwDwEOEOAB4i3AHAQ4Q7AHiIcAcAD/0/9e1ri3bbxSMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_image_path = \"./Data_122824/Glioma_MDC_2025_training_normalized/training0001.jpg\"\n",
    "example_json_path = \"./Data_122824/Glioma_MDC_2025_training_normalized/training0001.json\"\n",
    "\n",
    "\n",
    "overlay_image,crops=crops_image(example_image_path ,example_json_path)\n",
    "\n",
    "plt.imshow(crops[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1614\n",
      "1614\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_files=os.listdir(tr_folder_path)\n",
    "items = training_files\n",
    "jpg_dict = {re.search(r'\\d+', item).group(): item for item in items if item.endswith('.jpg')}\n",
    "json_dict = {re.search(r'\\d+', item).group(): item for item in items if item.endswith('.json')}\n",
    "\n",
    "# Find matching numbers\n",
    "matching_numbers = sorted(set(jpg_dict.keys()) & set(json_dict.keys()))\n",
    "\n",
    "# Create parallel lists\n",
    "jpg_files = [jpg_dict[num] for num in matching_numbers]\n",
    "json_files = [json_dict[num] for num in matching_numbers]\n",
    "\n",
    "all_image_patches=[]\n",
    "all_labels=[]\n",
    "\n",
    "for i in range(len(jpg_files)):\n",
    "    jpg_file=jpg_files[i]\n",
    "    json_file=json_files[i]\n",
    "    jpg_file=tr_folder_path+'/'+jpg_file\n",
    "    json_file=tr_folder_path+'/'+json_file\n",
    "\n",
    "    overlay_image,crops=crops_image(jpg_file,json_file)\n",
    "    labels=[]\n",
    "    with open(json_file,'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)  # Load JSON content into a Python dictionary\n",
    "        str_entries=json_data[\"shapes\"]\n",
    "        # print(str_entries)\n",
    "        \n",
    "        for entry in str_entries:\n",
    "            label=entry[\"label\"]\n",
    "            labels.append(label)\n",
    "\n",
    "    for j in range(len(crops)):\n",
    "        crop=crops[j]\n",
    "        label=labels[j]\n",
    "        if crop.shape[0]>5:\n",
    "            all_image_patches.append(crop)\n",
    "            all_labels.append(label)\n",
    "    # plt.imshow(overlay_image)\n",
    "    # plt.show()\n",
    "\n",
    "print(len(all_image_patches))\n",
    "print(len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels=[1 if label=='Mitosis' else 0 for label in all_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========================\n",
    "# Dataset Classes\n",
    "# ========================\n",
    "\n",
    "class PathologyDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        images: list of images (PIL Image or array)\n",
    "        labels: list of ground truth labels (0 or 1)\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        # If image is not a PIL Image, convert it.\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Inference Dataset\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# ========================\n",
    "# Helper Functions\n",
    "# ========================\n",
    "\n",
    "def compute_metrics(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score from logits and true labels.\n",
    "    \"\"\"\n",
    "    preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "    labels = labels.float()\n",
    "    \n",
    "    TP = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    FP = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    FN = ((preds == 0) & (labels == 1)).sum().item()\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "def get_transforms():\n",
    "    \"\"\"Define common transforms for training and inference.\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # ViT typically expects 224x224 images\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def build_vit_model(for_final=False):\n",
    "    \"\"\"\n",
    "    Instantiates a pretrained ViT model from torchvision, replaces the final \n",
    "    classification head with a binary classifier, and optionally freezes parameters.\n",
    "    \n",
    "    Args:\n",
    "        for_final (bool): If True, freeze all parameters except for the final layer.\n",
    "    Returns:\n",
    "        model (nn.Module): The modified ViT model.\n",
    "    \"\"\"\n",
    "    # Always start from the same pretrained weights.\n",
    "    model = models.vit_b_16(weights=\"ViT_B_16_Weights.DEFAULT\")\n",
    "    num_features = model.heads.head.in_features  # For torchvision ViT, classifier is under heads.head\n",
    "    model.heads.head = nn.Linear(num_features, 1)\n",
    "    # # Replace classification head\n",
    "\n",
    "    \n",
    "    # model=models.swin_t(weights=\"Swin_T_Weights.DEFAULT\")\n",
    "    # num_features = model.head.in_features  # For Swin, the classifier is in 'head'\n",
    "    # model.head = nn.Linear(num_features, 1)\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_initial_state(for_final=False):\n",
    "    \"\"\"\n",
    "    Returns the state dictionary of a freshly built model.\n",
    "    This allows all training runs to start from the same initialization.\n",
    "    \"\"\"\n",
    "    model = build_vit_model(for_final=for_final)\n",
    "    return model.state_dict()\n",
    "\n",
    "# ========================\n",
    "# Training Functions\n",
    "# ========================\n",
    "\n",
    "def train_transfer_vit_cv(image_list, label_list, num_epochs=50, batch_size=32, learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    Performs 10-fold cross validation on the provided dataset using a pretrained ViT.\n",
    "    Returns a dictionary with average validation loss, precision, recall, and F1.\n",
    "    \"\"\"\n",
    "    # Define transforms.\n",
    "    transform = get_transforms()\n",
    "    dataset = PathologyDataset(image_list, label_list, transform=transform)\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Get the common initial state for cross validation (for_final=False).\n",
    "    initial_state = get_initial_state(for_final=False)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_metrics = {\n",
    "        \"val_loss\": [],\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1\": []\n",
    "    }\n",
    "    \n",
    "    fold_idx = 1\n",
    "    for train_idx, val_idx in kf.split(np.arange(dataset_size)):\n",
    "        print(f\"\\nStarting fold {fold_idx}...\")\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset   = Subset(dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Create a fresh model and load the same initial weights.\n",
    "        model = build_vit_model(for_final=False)\n",
    "        model.load_state_dict(initial_state)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Define loss and optimizer.\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Training loop for current fold.\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            epoch_loss = running_loss / len(train_subset)\n",
    "            print(f\"Fold {fold_idx} | Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase for current fold.\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                all_preds.append(outputs)\n",
    "                all_labels.append(labels)\n",
    "        \n",
    "        val_loss /= len(val_subset)\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        precision, recall, f1 = compute_metrics(all_preds, all_labels)\n",
    "        \n",
    "        print(f\"Fold {fold_idx} | Val Loss: {val_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        fold_metrics[\"val_loss\"].append(val_loss)\n",
    "        fold_metrics[\"precision\"].append(precision)\n",
    "        fold_metrics[\"recall\"].append(recall)\n",
    "        fold_metrics[\"f1\"].append(f1)\n",
    "        \n",
    "        fold_idx += 1\n",
    "    \n",
    "    # Compute average metrics over all folds.\n",
    "    avg_val_loss = np.mean(fold_metrics[\"val_loss\"])\n",
    "    avg_precision = np.mean(fold_metrics[\"precision\"])\n",
    "    avg_recall = np.mean(fold_metrics[\"recall\"])\n",
    "    avg_f1 = np.mean(fold_metrics[\"f1\"])\n",
    "    \n",
    "    print(\"\\n10-Fold Cross Validation Results:\")\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"avg_val_loss\": avg_val_loss,\n",
    "        \"avg_precision\": avg_precision,\n",
    "        \"avg_recall\": avg_recall,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "\n",
    "def train_final_model(image_list, label_list, num_epochs=10, batch_size=32, learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    Trains the model on the entire dataset and returns the final trained model using ViT.\n",
    "    For the final training, the base layers are frozen except the final classification layer.\n",
    "    \"\"\"\n",
    "    transform = get_transforms()\n",
    "    dataset = PathologyDataset(image_list, label_list, transform=transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Get the common initial state for the final model (with final training settings).\n",
    "    initial_state = get_initial_state(for_final=True)\n",
    "    \n",
    "    model = build_vit_model(for_final=True)\n",
    "    model.load_state_dict(initial_state)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss and optimizer.\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    \n",
    "    # Training loop over the entire dataset.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataset)\n",
    "        print(f\"Final Model Training | Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_get_model(image_list, label_list, num_epochs_cv=10, num_epochs_final=10, batch_size=32, learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    Runs 5-fold cross validation to evaluate the ViT-based model, prints the averaged metrics,\n",
    "    then retrains the model on the entire dataset and returns the final model.\n",
    "    \"\"\"\n",
    "    print(\"Starting 5-fold cross validation...\")\n",
    "    cv_metrics = train_transfer_vit_cv(image_list, label_list,\n",
    "                                       num_epochs=num_epochs_cv,\n",
    "                                       batch_size=batch_size,\n",
    "                                       learning_rate=learning_rate)\n",
    "    \n",
    "    print(\"\\nRetraining final model on the entire dataset...\")\n",
    "    final_model = train_final_model(image_list, label_list,\n",
    "                                    num_epochs=num_epochs_final,\n",
    "                                    batch_size=batch_size,\n",
    "                                    learning_rate=learning_rate)\n",
    "    return final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold cross validation...\n",
      "\n",
      "Starting fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhixiu/anaconda3/envs/conch/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | Epoch 1/10 | Train Loss: 0.1661\n",
      "Fold 1 | Epoch 2/10 | Train Loss: 0.0435\n",
      "Fold 1 | Epoch 3/10 | Train Loss: 0.0342\n",
      "Fold 1 | Epoch 4/10 | Train Loss: 0.0164\n",
      "Fold 1 | Epoch 5/10 | Train Loss: 0.0081\n",
      "Fold 1 | Epoch 6/10 | Train Loss: 0.0037\n",
      "Fold 1 | Epoch 7/10 | Train Loss: 0.0019\n",
      "Fold 1 | Epoch 8/10 | Train Loss: 0.0012\n",
      "Fold 1 | Epoch 9/10 | Train Loss: 0.0008\n",
      "Fold 1 | Epoch 10/10 | Train Loss: 0.0006\n",
      "Fold 1 | Val Loss: 0.0850, Precision: 0.9945, Recall: 0.9731, F1: 0.9837\n",
      "\n",
      "Starting fold 2...\n",
      "Fold 2 | Epoch 1/10 | Train Loss: 0.1805\n",
      "Fold 2 | Epoch 2/10 | Train Loss: 0.0452\n",
      "Fold 2 | Epoch 3/10 | Train Loss: 0.0247\n",
      "Fold 2 | Epoch 4/10 | Train Loss: 0.0161\n",
      "Fold 2 | Epoch 5/10 | Train Loss: 0.0071\n",
      "Fold 2 | Epoch 6/10 | Train Loss: 0.0028\n",
      "Fold 2 | Epoch 7/10 | Train Loss: 0.0013\n",
      "Fold 2 | Epoch 8/10 | Train Loss: 0.0008\n",
      "Fold 2 | Epoch 9/10 | Train Loss: 0.0006\n",
      "Fold 2 | Epoch 10/10 | Train Loss: 0.0005\n",
      "Fold 2 | Val Loss: 0.1367, Precision: 0.9775, Recall: 0.9560, F1: 0.9667\n",
      "\n",
      "Starting fold 3...\n",
      "Fold 3 | Epoch 1/10 | Train Loss: 0.1660\n",
      "Fold 3 | Epoch 2/10 | Train Loss: 0.0489\n",
      "Fold 3 | Epoch 3/10 | Train Loss: 0.0264\n",
      "Fold 3 | Epoch 4/10 | Train Loss: 0.0155\n",
      "Fold 3 | Epoch 5/10 | Train Loss: 0.0064\n",
      "Fold 3 | Epoch 6/10 | Train Loss: 0.0025\n",
      "Fold 3 | Epoch 7/10 | Train Loss: 0.0013\n",
      "Fold 3 | Epoch 8/10 | Train Loss: 0.0009\n",
      "Fold 3 | Epoch 9/10 | Train Loss: 0.0007\n",
      "Fold 3 | Epoch 10/10 | Train Loss: 0.0005\n",
      "Fold 3 | Val Loss: 0.0388, Precision: 0.9945, Recall: 0.9890, F1: 0.9917\n",
      "\n",
      "Starting fold 4...\n",
      "Fold 4 | Epoch 1/10 | Train Loss: 0.1702\n",
      "Fold 4 | Epoch 2/10 | Train Loss: 0.0482\n",
      "Fold 4 | Epoch 3/10 | Train Loss: 0.0342\n",
      "Fold 4 | Epoch 4/10 | Train Loss: 0.0202\n",
      "Fold 4 | Epoch 5/10 | Train Loss: 0.0175\n",
      "Fold 4 | Epoch 6/10 | Train Loss: 0.0038\n",
      "Fold 4 | Epoch 7/10 | Train Loss: 0.0018\n",
      "Fold 4 | Epoch 8/10 | Train Loss: 0.0012\n",
      "Fold 4 | Epoch 9/10 | Train Loss: 0.0009\n",
      "Fold 4 | Epoch 10/10 | Train Loss: 0.0007\n",
      "Fold 4 | Val Loss: 0.0594, Precision: 0.9881, Recall: 0.9881, F1: 0.9881\n",
      "\n",
      "Starting fold 5...\n",
      "Fold 5 | Epoch 1/10 | Train Loss: 0.1968\n",
      "Fold 5 | Epoch 2/10 | Train Loss: 0.0465\n",
      "Fold 5 | Epoch 3/10 | Train Loss: 0.0360\n",
      "Fold 5 | Epoch 4/10 | Train Loss: 0.0177\n",
      "Fold 5 | Epoch 5/10 | Train Loss: 0.0069\n",
      "Fold 5 | Epoch 6/10 | Train Loss: 0.0028\n",
      "Fold 5 | Epoch 7/10 | Train Loss: 0.0014\n",
      "Fold 5 | Epoch 8/10 | Train Loss: 0.0009\n",
      "Fold 5 | Epoch 9/10 | Train Loss: 0.0006\n",
      "Fold 5 | Epoch 10/10 | Train Loss: 0.0005\n",
      "Fold 5 | Val Loss: 0.0615, Precision: 0.9821, Recall: 0.9706, F1: 0.9763\n",
      "\n",
      "10-Fold Cross Validation Results:\n",
      "Average Validation Loss: 0.0763\n",
      "Average Precision: 0.9873\n",
      "Average Recall: 0.9754\n",
      "Average F1 Score: 0.9813\n",
      "\n",
      "Retraining final model on the entire dataset...\n",
      "Final Model Training | Epoch 1/10 | Loss: 0.1459\n",
      "Final Model Training | Epoch 2/10 | Loss: 0.0461\n",
      "Final Model Training | Epoch 3/10 | Loss: 0.0334\n",
      "Final Model Training | Epoch 4/10 | Loss: 0.0196\n",
      "Final Model Training | Epoch 5/10 | Loss: 0.0097\n",
      "Final Model Training | Epoch 6/10 | Loss: 0.0046\n",
      "Final Model Training | Epoch 7/10 | Loss: 0.0019\n",
      "Final Model Training | Epoch 8/10 | Loss: 0.0011\n",
      "Final Model Training | Epoch 9/10 | Loss: 0.0007\n",
      "Final Model Training | Epoch 10/10 | Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_and_get_model(all_image_patches, all_labels, batch_size=16, learning_rate=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
